{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load_metric(\"sacrebleu\")\n",
    "chrf = load_metric('chrf')\n",
    "bertscore_metric = load_metric('bertscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(\"../config.json\"), \"Config file not found! Add a config.json file in ./src folder.\"\n",
    "with open(\"../config.json\", \"r\") as f:\n",
    "    __config__ = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/processed/train.csv\")\n",
    "validation_df = pd.read_csv(\"../data/processed/validation.csv\")\n",
    "test_df = pd.read_csv(\"../data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([train_df, validation_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response_file_name = __config__[\"gpt4_turbo\"][\"response_save_path\"]\n",
    "predictions_file_name = __config__[\"gpt4_turbo\"][\"prediction_df_save_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(api_key=\"Enter Your Key\")\n",
    "def make_api_call(query):\n",
    "  completion = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'Translate the following sentences from Hindi-English (Hinglish) to English'},\n",
    "        {'role': 'user', 'content': query},\n",
    "    ]\n",
    "  )\n",
    "  print(completion.choices[0].message)\n",
    "  return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[500, \"hi_en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_1 = []\n",
    "for idx, sent in test_df.loc[:375, \"hi_en\"].iteritems():\n",
    "    preds_1.append(make_api_call(sent).content)\n",
    "\n",
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    # Iterate over each element in the list\n",
    "    for item in preds_1:\n",
    "        # Write each item to the file followed by a newline\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_376 = []\n",
    "for idx, sent in test_df.loc[376:499, \"hi_en\"].iteritems():\n",
    "    preds_376.append(make_api_call(sent).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    # Iterate over each element in the list\n",
    "    for item in preds_376:\n",
    "        # Write each item to the file followed by a newline\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for idx, sent in test_df.loc[500:1001, \"hi_en\"].iteritems():\n",
    "    preds.append(make_api_call(sent).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    # Iterate over each element in the list\n",
    "    for item in preds:\n",
    "        # Write each item to the file followed by a newline\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_1002 = []\n",
    "for idx, sent in test_df.loc[1002:1500, \"hi_en\"].iteritems():\n",
    "    preds_1002.append(make_api_call(sent).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    # Iterate over each element in the list\n",
    "    for item in preds_1002:\n",
    "        # Write each item to the file followed by a newline\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_1501 = []\n",
    "for idx, sent in test_df.loc[1501:2000, \"hi_en\"].iteritems():\n",
    "    preds_1501.append(make_api_call(sent).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    # Iterate over each element in the list\n",
    "    for item in preds_1501:\n",
    "        # Write each item to the file followed by a newline\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_2001 = []\n",
    "for idx, sent in test_df.loc[2001:2500, \"hi_en\"].iteritems():\n",
    "    preds_2001.append(make_api_call(sent).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    # Iterate over each element in the list\n",
    "    for item in preds_2001:\n",
    "        # Write each item to the file followed by a newline\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_2501 = []\n",
    "for idx, sent in test_df.loc[2501:3000, \"hi_en\"].iteritems():\n",
    "    preds_2501.append(make_api_call(sent).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    # Iterate over each element in the list\n",
    "    for item in preds_2501:\n",
    "        # Write each item to the file followed by a newline\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_3001 = []\n",
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    for idx, sent in test_df.loc[3001:3066, \"hi_en\"].iteritems():\n",
    "        print(idx)\n",
    "        item = make_api_call(sent).content\n",
    "        preds_3001.append(item)\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_3001 = []\n",
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    for idx, sent in test_df.loc[3066:3600, \"hi_en\"].iteritems():\n",
    "        print(idx)\n",
    "        item = make_api_call(sent).content\n",
    "        preds_3001.append(item)\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_3601 = []\n",
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    for idx, sent in test_df.loc[3601:4200, \"hi_en\"].iteritems():\n",
    "        print(idx)\n",
    "        item = make_api_call(sent).content\n",
    "        preds_3601.append(item)\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_4201 = []\n",
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    for idx, sent in test_df.loc[4201:5000, \"hi_en\"].iteritems():\n",
    "        print(idx)\n",
    "        item = make_api_call(sent).content\n",
    "        preds_4201.append(item)\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_5001 = []\n",
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    for idx, sent in test_df.loc[5001:5700, \"hi_en\"].iteritems():\n",
    "        print(idx)\n",
    "        item = make_api_call(sent).content\n",
    "        preds_5001.append(item)\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_5701 = []\n",
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    for idx, sent in test_df.loc[5701:6300, \"hi_en\"].iteritems():\n",
    "        print(idx)\n",
    "        item = make_api_call(sent).content\n",
    "        preds_5701.append(item)\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_6301 = []\n",
    "with open(api_response_file_name, 'a', encoding=\"utf-8\") as file:\n",
    "    for idx, sent in test_df.loc[6301:, \"hi_en\"].iteritems():\n",
    "        print(idx)\n",
    "        item = make_api_call(sent).content\n",
    "        preds_6301.append(item)\n",
    "        item_without_newline = item.replace('\\n', '')\n",
    "        file.write(item_without_newline + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_3001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(api_response_file_name, 'r', encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# You may want to strip newline characters from each line\n",
    "lines = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = test_df.head(len(lines)).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[\"preds\"] = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [[ref.split()] for ref in df_preds['en']]\n",
    "predictions = [pred.split() for pred in df_preds['preds']]\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"BLEU Score:\", bleu_score['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_references = [[i] for i in df_preds['en'].tolist()]\n",
    "all_predictions = df_preds['preds'].tolist()\n",
    "# Calculate the overall chrF score\n",
    "overall_chrf_score = chrf.compute(predictions=all_predictions, references=all_references)\n",
    "\n",
    "print(\"Overall chrF Score:\", overall_chrf_score[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = df_preds['en'].tolist()\n",
    "predictions = df_preds['preds'].tolist()\n",
    "\n",
    "# Calculate BERTScore for each pair of texts\n",
    "scores = bertscore_metric.compute(predictions=predictions, references=references, lang='en')\n",
    "# Calculate the average scores\n",
    "average_precision = sum(scores['precision']) / len(scores['precision'])\n",
    "average_recall = sum(scores['recall']) / len(scores['recall'])\n",
    "average_f1 = sum(scores['f1']) / len(scores['f1'])\n",
    "\n",
    "print(f\"Bert Precision: {average_precision}\")\n",
    "print(f\"Bert Recall: {average_recall}\")\n",
    "print(f\"Bert F1: {average_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.to_csv(predictions_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source_df = pd.read_csv(\"../data/processed/test_source.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df_preds.merge(test_source_df[['en', 'source']], on='en', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cmu_hinglish_dog = merged_df[merged_df['source'] == 'cmu_hinglish_dog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cmu_hinglish_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = merged_df[merged_df['source'] == 'top']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [[ref.split()] for ref in df_cmu_hinglish_dog['en']]\n",
    "predictions = [pred.split() for pred in df_cmu_hinglish_dog['preds']]\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"CMU Hinglish Dog BLEU Score:\", bleu_score['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_references = [[i] for i in df_cmu_hinglish_dog['en'].tolist()]\n",
    "all_predictions = df_cmu_hinglish_dog['preds'].tolist()\n",
    "# Calculate the overall chrF score\n",
    "overall_chrf_score = chrf.compute(predictions=all_predictions, references=all_references)\n",
    "\n",
    "print(\"CMU Hinglish Dog chrF Score:\", overall_chrf_score[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = df_cmu_hinglish_dog['en'].tolist()\n",
    "predictions = df_cmu_hinglish_dog['preds'].tolist()\n",
    "\n",
    "# Calculate BERTScore for each pair of texts\n",
    "scores = bertscore_metric.compute(predictions=predictions, references=references, lang='en')\n",
    "# Calculate the average scores\n",
    "average_precision = sum(scores['precision']) / len(scores['precision'])\n",
    "average_recall = sum(scores['recall']) / len(scores['recall'])\n",
    "average_f1 = sum(scores['f1']) / len(scores['f1'])\n",
    "\n",
    "print(f\"CMU Hinglish Dog Bert Precision: {average_precision}\")\n",
    "print(f\"CMU Hinglish Dog Bert Recall: {average_recall}\")\n",
    "print(f\"CMU Hinglish Dog Bert F1: {average_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [[ref.split()] for ref in df_top['en']]\n",
    "predictions = [pred.split() for pred in df_top['preds']]\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"Top BLEU Score:\", bleu_score['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_references = [[i] for i in df_top['en'].tolist()]\n",
    "all_predictions = df_top['preds'].tolist()\n",
    "# Calculate the overall chrF score\n",
    "overall_chrf_score = chrf.compute(predictions=all_predictions, references=all_references)\n",
    "\n",
    "print(\"Top chrF Score:\", overall_chrf_score[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = df_top['en'].tolist()\n",
    "predictions = df_top['preds'].tolist()\n",
    "\n",
    "# Calculate BERTScore for each pair of texts\n",
    "scores = bertscore_metric.compute(predictions=predictions, references=references, lang='en')\n",
    "# Calculate the average scores\n",
    "average_precision = sum(scores['precision']) / len(scores['precision'])\n",
    "average_recall = sum(scores['recall']) / len(scores['recall'])\n",
    "average_f1 = sum(scores['f1']) / len(scores['f1'])\n",
    "\n",
    "print(f\"Top Bert Precision: {average_precision}\")\n",
    "print(f\"Top Bert Recall: {average_recall}\")\n",
    "print(f\"Top Bert F1: {average_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

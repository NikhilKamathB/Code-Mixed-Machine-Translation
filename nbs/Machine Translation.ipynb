{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "a797b411-ba32-4a49-8254-fd3b57caf76c",
         "metadata": {},
         "source": [
            "## Importing libraries and modules"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "1bd2df09-6a25-4686-b1ff-392d729c4030",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Enable auto-reload\n",
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 33,
         "id": "ddc02188-33e9-4af6-877e-7b343c69de5f",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Suppress warnings\n",
            "import warnings\n",
            "warnings.filterwarnings(\"ignore\")\n",
            "\n",
            "# Add one level up directory to the path\n",
            "import sys\n",
            "sys.path.append(\"..\")\n",
            "\n",
            "# Import libraries\n",
            "import torch\n",
            "import random\n",
            "\n",
            "# Import custom modules\n",
            "from src.data import *\n",
            "from src.machine_translation import *\n",
            "from src.data.utils import get_dataset\n",
            "from src.data.preprocess import clean_text\n",
            "from src.data.tokenizer import CustomBartTokenizer\n",
            "from src.machine_translation.net import CodeMixedModel, CodeMixedModelHGTrainer\n",
            "from src.machine_translation.utils import get_data_loader_models, get_tokenized_dataset\n",
            "from src.machine_translation.models.bart_conditional import BartForConditionalGeneration"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "7afb9724-cdf5-458c-9ba9-854da0b39806",
         "metadata": {},
         "source": [
            "## Data acquisition, cleaning and processing"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c22ec356-0bbb-4f7b-8aa7-b7d5d7ba8879",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Found cached dataset cmu_hinglish_dog (C:/Users/tejam/.cache/huggingface/datasets/cmu_hinglish_dog/default/0.0.0/a646ab55bde6539dc76686b3b758d0e6ad2a1213f05a69e85eaa4e55bb20ddad)\n",
                  "100%|██████████| 3/3 [00:00<00:00, 76.90it/s]\n",
                  "Found cached dataset json (C:/Users/tejam/.cache/huggingface/datasets/findnitai___json/findnitai--english-to-hinglish-85f0c6597edef310/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
                  "100%|██████████| 1/1 [00:00<00:00, 14.59it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Wall time: 8.2 s\n"
               ]
            }
         ],
         "source": [
            "%%time\n",
            "train_df, validation_df, test_df = get_dataset()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "#train_df.en.info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "65e76ede-e303-4558-8f66-810de6ef9330",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>hi_en</th>\n",
                     "      <th>en</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>HELLO, KYA AAP KO MOVIES PASAND HEIN?</td>\n",
                     "      <td>Hello. Do you like movies?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>HAAN, OLD TOYS KE BASED HO THO PASAND HEIN, TH...</td>\n",
                     "      <td>Yes, but ones based on old toys are werid</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>REAL STEEL THO YAAD HEIN, MEIN DEKHA HOON</td>\n",
                     "      <td>This one is called Real Steel and I trying to ...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>MEIN NAHI, MUJE KOI INTEREST NAHI HEI</td>\n",
                     "      <td>I haven't, no interest in it</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>MEIN MAANTHA HOON, ROCKY KA REVIEWER KO US ROB...</td>\n",
                     "      <td>I agree with the reviewer stating it's Rocky w...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>...</th>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>8055</th>\n",
                     "      <td>Thik hai\\n</td>\n",
                     "      <td>Ok.</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>8056</th>\n",
                     "      <td>Thik hai bhai\\n</td>\n",
                     "      <td>ok bro</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>8057</th>\n",
                     "      <td>Kya ham chalu kar sakte hai?\\n</td>\n",
                     "      <td>shall we continue?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>8058</th>\n",
                     "      <td>Kya aapko pasand hai hamare saath\\n</td>\n",
                     "      <td>do you like we can</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>8059</th>\n",
                     "      <td>Haa mere pass pandrah thanks hai\\n</td>\n",
                     "      <td>Yeah I have fifteen thanks.</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "<p>174443 rows × 2 columns</p>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                                                  hi_en  \\\n",
                     "0                 HELLO, KYA AAP KO MOVIES PASAND HEIN?   \n",
                     "1     HAAN, OLD TOYS KE BASED HO THO PASAND HEIN, TH...   \n",
                     "2             REAL STEEL THO YAAD HEIN, MEIN DEKHA HOON   \n",
                     "3                 MEIN NAHI, MUJE KOI INTEREST NAHI HEI   \n",
                     "4     MEIN MAANTHA HOON, ROCKY KA REVIEWER KO US ROB...   \n",
                     "...                                                 ...   \n",
                     "8055                                         Thik hai\\n   \n",
                     "8056                                    Thik hai bhai\\n   \n",
                     "8057                     Kya ham chalu kar sakte hai?\\n   \n",
                     "8058                Kya aapko pasand hai hamare saath\\n   \n",
                     "8059                 Haa mere pass pandrah thanks hai\\n   \n",
                     "\n",
                     "                                                     en  \n",
                     "0                            Hello. Do you like movies?  \n",
                     "1             Yes, but ones based on old toys are werid  \n",
                     "2     This one is called Real Steel and I trying to ...  \n",
                     "3                          I haven't, no interest in it  \n",
                     "4     I agree with the reviewer stating it's Rocky w...  \n",
                     "...                                                 ...  \n",
                     "8055                                                Ok.  \n",
                     "8056                                             ok bro  \n",
                     "8057                                 shall we continue?  \n",
                     "8058                                 do you like we can  \n",
                     "8059                        Yeah I have fifteen thanks.  \n",
                     "\n",
                     "[174443 rows x 2 columns]"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "train_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b4136fec-19ad-4643-83c6-3301a0388e2f",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>hi_en</th>\n",
                     "      <th>en</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>hello</td>\n",
                     "      <td>hello</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>hello yar, mein is movie ko nahi dekha hoon th...</td>\n",
                     "      <td>hello there, I have not seen this movie so im ...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>acha tho is movie kis baare me hein?</td>\n",
                     "      <td>Alright that is fine. What is the movie?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>is movie tho social network ke bare mein hein</td>\n",
                     "      <td>The movie is The Social Network</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>mein aise kuch nahi dekha hoon</td>\n",
                     "      <td>I have not seen that one either.</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>...</th>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>936</th>\n",
                     "      <td>Sunkar good movie hai. Kya ham finish kar skat...</td>\n",
                     "      <td>Sounds like a good movie. Can we finish now?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>937</th>\n",
                     "      <td>Yep. Thanks baat karne ke liye\\n</td>\n",
                     "      <td>Yep. Thanks for chatting</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>938</th>\n",
                     "      <td>Thanks, mei dekhati hui. Achi baat hai\\n</td>\n",
                     "      <td>thanks, I will watch it. SOunds good</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>940</th>\n",
                     "      <td>kya tumhe movie Despicable Me pasand hai?\\n</td>\n",
                     "      <td>Did you like the movie Despicable Me?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>941</th>\n",
                     "      <td>mujhe nahi lagta ki ye movie 2010 me aayi thi,...</td>\n",
                     "      <td>I did not realize this movie came out in 2010,...</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "<p>3243 rows × 2 columns</p>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                                                 hi_en  \\\n",
                     "0                                                hello   \n",
                     "1    hello yar, mein is movie ko nahi dekha hoon th...   \n",
                     "2                 acha tho is movie kis baare me hein?   \n",
                     "3        is movie tho social network ke bare mein hein   \n",
                     "4                       mein aise kuch nahi dekha hoon   \n",
                     "..                                                 ...   \n",
                     "936  Sunkar good movie hai. Kya ham finish kar skat...   \n",
                     "937                   Yep. Thanks baat karne ke liye\\n   \n",
                     "938           Thanks, mei dekhati hui. Achi baat hai\\n   \n",
                     "940        kya tumhe movie Despicable Me pasand hai?\\n   \n",
                     "941  mujhe nahi lagta ki ye movie 2010 me aayi thi,...   \n",
                     "\n",
                     "                                                    en  \n",
                     "0                                                hello  \n",
                     "1    hello there, I have not seen this movie so im ...  \n",
                     "2             Alright that is fine. What is the movie?  \n",
                     "3                      The movie is The Social Network  \n",
                     "4                     I have not seen that one either.  \n",
                     "..                                                 ...  \n",
                     "936       Sounds like a good movie. Can we finish now?  \n",
                     "937                           Yep. Thanks for chatting  \n",
                     "938               thanks, I will watch it. SOunds good  \n",
                     "940              Did you like the movie Despicable Me?  \n",
                     "941  I did not realize this movie came out in 2010,...  \n",
                     "\n",
                     "[3243 rows x 2 columns]"
                  ]
               },
               "execution_count": 6,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "validation_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8f002715-2359-4b51-b7b9-13e0c2738b21",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>hi_en</th>\n",
                     "      <th>en</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>Hello! Kaise ho? Tumne recently koi achhi movi...</td>\n",
                     "      <td>Hello! How are you? Have you seen any good mov...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>Tumne recently koi achhi movie dekhi toh uske ...</td>\n",
                     "      <td>Can you tell me the name of any good movie you...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>Hello!</td>\n",
                     "      <td>Hello!</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>Haan, maine abhi La La Land dekhi.</td>\n",
                     "      <td>Yes, I just watched La La Land</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>Tumne dekhi hai?</td>\n",
                     "      <td>Have you seen it?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>...</th>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6508</th>\n",
                     "      <td>alarm ko abhi stop kare</td>\n",
                     "      <td>Stop alarm now</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6509</th>\n",
                     "      <td>Har ghante ke liye alarm set kare</td>\n",
                     "      <td>set alarm every hour</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6510</th>\n",
                     "      <td>Bobby ko text kare</td>\n",
                     "      <td>text Bobby</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6511</th>\n",
                     "      <td>Muje shaam 6 baje laundry ko pick up karne ke ...</td>\n",
                     "      <td>remind me to pick up laundry at 6 pm</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6512</th>\n",
                     "      <td>Kya aap muje har subah 6 baje apne baccho ko j...</td>\n",
                     "      <td>Can you remind me to wake my kids up every mor...</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "<p>7072 rows × 2 columns</p>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                                                  hi_en  \\\n",
                     "0     Hello! Kaise ho? Tumne recently koi achhi movi...   \n",
                     "1     Tumne recently koi achhi movie dekhi toh uske ...   \n",
                     "2                                                Hello!   \n",
                     "3                    Haan, maine abhi La La Land dekhi.   \n",
                     "4                                      Tumne dekhi hai?   \n",
                     "...                                                 ...   \n",
                     "6508                            alarm ko abhi stop kare   \n",
                     "6509                  Har ghante ke liye alarm set kare   \n",
                     "6510                                 Bobby ko text kare   \n",
                     "6511  Muje shaam 6 baje laundry ko pick up karne ke ...   \n",
                     "6512  Kya aap muje har subah 6 baje apne baccho ko j...   \n",
                     "\n",
                     "                                                     en  \n",
                     "0     Hello! How are you? Have you seen any good mov...  \n",
                     "1     Can you tell me the name of any good movie you...  \n",
                     "2                                               Hello!   \n",
                     "3                        Yes, I just watched La La Land  \n",
                     "4                                     Have you seen it?  \n",
                     "...                                                 ...  \n",
                     "6508                                     Stop alarm now  \n",
                     "6509                               set alarm every hour  \n",
                     "6510                                         text Bobby  \n",
                     "6511               remind me to pick up laundry at 6 pm  \n",
                     "6512  Can you remind me to wake my kids up every mor...  \n",
                     "\n",
                     "[7072 rows x 2 columns]"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "test_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "9e93f1af-c168-4d30-8d38-f08a1245b594",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Wall time: 3.26 s\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>hi_en</th>\n",
                     "      <th>en</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>hello, kya aap ko movies pasand hein?</td>\n",
                     "      <td>hello. do you like movies?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>haan, old toys ke based ho tho pasand hein, th...</td>\n",
                     "      <td>yes, but ones based on old toys are werid</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>real steel tho yaad hein, mein dekha hoon</td>\n",
                     "      <td>this one is called real steel and i trying to ...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>mein nahi, muje koi interest nahi hei</td>\n",
                     "      <td>i haven't, no interest in it</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>mein maantha hoon, rocky ka reviewer ko us rob...</td>\n",
                     "      <td>i agree with the reviewer stating it's rocky w...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>...</th>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>8055</th>\n",
                     "      <td>thik hai</td>\n",
                     "      <td>ok.</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>8056</th>\n",
                     "      <td>thik hai bhai</td>\n",
                     "      <td>ok bro</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>8057</th>\n",
                     "      <td>kya ham chalu kar sakte hai?</td>\n",
                     "      <td>shall we continue?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>8058</th>\n",
                     "      <td>kya aapko pasand hai hamare saath</td>\n",
                     "      <td>do you like we can</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>8059</th>\n",
                     "      <td>haa mere pass pandrah thanks hai</td>\n",
                     "      <td>yeah i have fifteen thanks.</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "<p>174443 rows × 2 columns</p>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                                                  hi_en  \\\n",
                     "0                 hello, kya aap ko movies pasand hein?   \n",
                     "1     haan, old toys ke based ho tho pasand hein, th...   \n",
                     "2             real steel tho yaad hein, mein dekha hoon   \n",
                     "3                 mein nahi, muje koi interest nahi hei   \n",
                     "4     mein maantha hoon, rocky ka reviewer ko us rob...   \n",
                     "...                                                 ...   \n",
                     "8055                                           thik hai   \n",
                     "8056                                      thik hai bhai   \n",
                     "8057                       kya ham chalu kar sakte hai?   \n",
                     "8058                  kya aapko pasand hai hamare saath   \n",
                     "8059                   haa mere pass pandrah thanks hai   \n",
                     "\n",
                     "                                                     en  \n",
                     "0                            hello. do you like movies?  \n",
                     "1             yes, but ones based on old toys are werid  \n",
                     "2     this one is called real steel and i trying to ...  \n",
                     "3                          i haven't, no interest in it  \n",
                     "4     i agree with the reviewer stating it's rocky w...  \n",
                     "...                                                 ...  \n",
                     "8055                                                ok.  \n",
                     "8056                                             ok bro  \n",
                     "8057                                 shall we continue?  \n",
                     "8058                                 do you like we can  \n",
                     "8059                        yeah i have fifteen thanks.  \n",
                     "\n",
                     "[174443 rows x 2 columns]"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "%%time\n",
            "train_df = train_df.applymap(clean_text)\n",
            "train_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "1016d417-f744-4caa-9773-e968eafc35c8",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Wall time: 69.5 ms\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>hi_en</th>\n",
                     "      <th>en</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>hello</td>\n",
                     "      <td>hello</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>hello yar, mein is movie ko nahi dekha hoon th...</td>\n",
                     "      <td>hello there, i have not seen this movie so im ...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>acha tho is movie kis baare me hein?</td>\n",
                     "      <td>alright that is fine. what is the movie?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>is movie tho social network ke bare mein hein</td>\n",
                     "      <td>the movie is the social network</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>mein aise kuch nahi dekha hoon</td>\n",
                     "      <td>i have not seen that one either.</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>...</th>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>936</th>\n",
                     "      <td>sunkar good movie hai. kya ham finish kar skat...</td>\n",
                     "      <td>sounds like a good movie. can we finish now?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>937</th>\n",
                     "      <td>yep. thanks baat karne ke liye</td>\n",
                     "      <td>yep. thanks for chatting</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>938</th>\n",
                     "      <td>thanks, mei dekhati hui. achi baat hai</td>\n",
                     "      <td>thanks, i will watch it. sounds good</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>940</th>\n",
                     "      <td>kya tumhe movie despicable me pasand hai?</td>\n",
                     "      <td>did you like the movie despicable me?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>941</th>\n",
                     "      <td>mujhe nahi lagta ki ye movie 2010 me aayi thi,...</td>\n",
                     "      <td>i did not realize this movie came out in 2010,...</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "<p>3243 rows × 2 columns</p>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                                                 hi_en  \\\n",
                     "0                                                hello   \n",
                     "1    hello yar, mein is movie ko nahi dekha hoon th...   \n",
                     "2                 acha tho is movie kis baare me hein?   \n",
                     "3        is movie tho social network ke bare mein hein   \n",
                     "4                       mein aise kuch nahi dekha hoon   \n",
                     "..                                                 ...   \n",
                     "936  sunkar good movie hai. kya ham finish kar skat...   \n",
                     "937                     yep. thanks baat karne ke liye   \n",
                     "938             thanks, mei dekhati hui. achi baat hai   \n",
                     "940          kya tumhe movie despicable me pasand hai?   \n",
                     "941  mujhe nahi lagta ki ye movie 2010 me aayi thi,...   \n",
                     "\n",
                     "                                                    en  \n",
                     "0                                                hello  \n",
                     "1    hello there, i have not seen this movie so im ...  \n",
                     "2             alright that is fine. what is the movie?  \n",
                     "3                      the movie is the social network  \n",
                     "4                     i have not seen that one either.  \n",
                     "..                                                 ...  \n",
                     "936       sounds like a good movie. can we finish now?  \n",
                     "937                           yep. thanks for chatting  \n",
                     "938               thanks, i will watch it. sounds good  \n",
                     "940              did you like the movie despicable me?  \n",
                     "941  i did not realize this movie came out in 2010,...  \n",
                     "\n",
                     "[3243 rows x 2 columns]"
                  ]
               },
               "execution_count": 9,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "%%time\n",
            "validation_df = validation_df.applymap(clean_text)\n",
            "validation_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "46573bf4-a9cc-49d5-9659-3966fc9a80b9",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Wall time: 138 ms\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>hi_en</th>\n",
                     "      <th>en</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>hello! kaise ho? tumne recently koi achhi movi...</td>\n",
                     "      <td>hello! how are you? have you seen any good mov...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>tumne recently koi achhi movie dekhi toh uske ...</td>\n",
                     "      <td>can you tell me the name of any good movie you...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>hello!</td>\n",
                     "      <td>hello!</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>haan, maine abhi la la land dekhi.</td>\n",
                     "      <td>yes, i just watched la la land</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>tumne dekhi hai?</td>\n",
                     "      <td>have you seen it?</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>...</th>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6508</th>\n",
                     "      <td>alarm ko abhi stop kare</td>\n",
                     "      <td>stop alarm now</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6509</th>\n",
                     "      <td>har ghante ke liye alarm set kare</td>\n",
                     "      <td>set alarm every hour</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6510</th>\n",
                     "      <td>bobby ko text kare</td>\n",
                     "      <td>text bobby</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6511</th>\n",
                     "      <td>muje shaam 6 baje laundry ko pick up karne ke ...</td>\n",
                     "      <td>remind me to pick up laundry at 6 pm</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6512</th>\n",
                     "      <td>kya aap muje har subah 6 baje apne baccho ko j...</td>\n",
                     "      <td>can you remind me to wake my kids up every mor...</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "<p>7072 rows × 2 columns</p>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                                                  hi_en  \\\n",
                     "0     hello! kaise ho? tumne recently koi achhi movi...   \n",
                     "1     tumne recently koi achhi movie dekhi toh uske ...   \n",
                     "2                                                hello!   \n",
                     "3                    haan, maine abhi la la land dekhi.   \n",
                     "4                                      tumne dekhi hai?   \n",
                     "...                                                 ...   \n",
                     "6508                            alarm ko abhi stop kare   \n",
                     "6509                  har ghante ke liye alarm set kare   \n",
                     "6510                                 bobby ko text kare   \n",
                     "6511  muje shaam 6 baje laundry ko pick up karne ke ...   \n",
                     "6512  kya aap muje har subah 6 baje apne baccho ko j...   \n",
                     "\n",
                     "                                                     en  \n",
                     "0     hello! how are you? have you seen any good mov...  \n",
                     "1     can you tell me the name of any good movie you...  \n",
                     "2                                                hello!  \n",
                     "3                        yes, i just watched la la land  \n",
                     "4                                     have you seen it?  \n",
                     "...                                                 ...  \n",
                     "6508                                     stop alarm now  \n",
                     "6509                               set alarm every hour  \n",
                     "6510                                         text bobby  \n",
                     "6511               remind me to pick up laundry at 6 pm  \n",
                     "6512  can you remind me to wake my kids up every mor...  \n",
                     "\n",
                     "[7072 rows x 2 columns]"
                  ]
               },
               "execution_count": 10,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "%%time\n",
            "test_df = test_df.applymap(clean_text)\n",
            "test_df"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "6815fbe5-c088-40b5-9406-2c942a6d09f8",
         "metadata": {},
         "source": [
            "## Data tokenization"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "5a5dbb69-5bde-412d-9071-9e6c3ef6a25e",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
               ]
            }
         ],
         "source": [
            "bart_tokenizer = CustomBartTokenizer().build()\n",
            "bart_tokenizer_scratch = CustomBartTokenizer().build(data=train_df[\"hi_en\"], tokenizer_style=STYLE.SCRATCH.value)\n",
            "bart_tokenizer_append = CustomBartTokenizer().build(data=train_df[\"hi_en\"], tokenizer_style=STYLE.APPEND.value)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d8d171f9-4ec2-4458-bd87-70cd7d7a280a",
         "metadata": {},
         "outputs": [],
         "source": [
            "query = \"Hello! Ye CSCI 544 ka project he. Ye project Code-Mixed Machine Translation ke bara me he. Hamane hamari khoon bahahe.\"\n",
            "query = clean_text(text=query)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8049c0ea-6b74-428f-8ec9-f9b578641d53",
         "metadata": {},
         "outputs": [],
         "source": [
            "token_idx = bart_tokenizer.encode(query, add_special_tokens=True)\n",
            "token_word = bart_tokenizer.convert_ids_to_tokens(token_idx)\n",
            "print('#'*100)\n",
            "print(\"This is an example of using the default Bart Tokenizer.\\n\")\n",
            "print(f\"Sentence: {query}\\n\")\n",
            "print(f\"Tokenized word: {token_word}\\n\")\n",
            "print(f\"Tokenized idx: {token_idx}\")\n",
            "print('#'*100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "983d5c73-96b4-4c61-af57-46ecd0891993",
         "metadata": {},
         "outputs": [],
         "source": [
            "token_idx = bart_tokenizer_scratch.encode(query, add_special_tokens=True)\n",
            "token_word = bart_tokenizer_scratch.convert_ids_to_tokens(token_idx)\n",
            "print('#'*100)\n",
            "print(\"This is an example of the Bart Tokenizer that is trained from scratch.\\n\")\n",
            "print(f\"Sentence: {query}\\n\")\n",
            "print(f\"Tokenized word: {token_word}\\n\")\n",
            "print(f\"Tokenized idx: {token_idx}\")\n",
            "print('#'*100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "122936f1-df9e-4342-9492-9dec2cb289bc",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "####################################################################################################\n",
                  "This is an example of the Bart Tokenizer to which we append our new data.\n",
                  "\n",
                  "Sentence: hello! ye csci 544 ka project he. ye project code-mixed machine translation ke bara me he. hamane hamari khoon bahahe.\n",
                  "\n",
                  "Tokenized word: ['<s>', 'hello', '!', 'Ġye', 'csc', 'i', 'Ġ5', '44', 'Ġka', 'Ġproject', 'Ġhe', '.', 'Ġye', 'Ġproject', 'Ġcode', '-', 'm', 'ixed', 'mach', 'ine', 'tran', 'sla', 'tion', 'Ġke', 'Ġbar', 'a', 'Ġme', 'Ġhe', '.', 'hamane', 'hamari', 'khoo', 'n', 'baha', 'he', '.', '</s>']\n",
                  "\n",
                  "Tokenized idx: [0, 42891, 328, 32440, 79705, 118, 195, 3305, 4661, 695, 37, 4, 32440, 695, 3260, 12, 119, 24194, 69287, 833, 55726, 52487, 24659, 7321, 2003, 102, 162, 37, 4, 72070, 73355, 66618, 282, 80387, 700, 4, 2]\n",
                  "####################################################################################################\n"
               ]
            }
         ],
         "source": [
            "token_idx = bart_tokenizer_append.encode(query, add_special_tokens=True)\n",
            "token_word = bart_tokenizer_append.convert_ids_to_tokens(token_idx)\n",
            "print('#'*100)\n",
            "print(\"This is an example of the Bart Tokenizer to which we append our new data.\\n\")\n",
            "print(f\"Sentence: {query}\\n\")\n",
            "print(f\"Tokenized word: {token_word}\\n\")\n",
            "print(f\"Tokenized idx: {token_idx}\")\n",
            "print('#'*100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "390d4005-a4fe-4187-8a41-a987029417b1",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
               ]
            }
         ],
         "source": [
            "# Now that we know how to get a BART-Tokenizer - default, appeded and scratch, let us get the tokenizer for our code-mixed language\n",
            "# and the target language.\n",
            "# Code-Mixed Language - Build a Bart-Tokenizer from scratch.\n",
            "# Target Language (English) - Use the default Bart-Tokenizer.\n",
            "hi_en_bart_tokenizer = CustomBartTokenizer().build(\n",
            "    data=train_df[\"hi_en\"],\n",
            "    tokenizer_style=MBART_TOKENIZER_BPE_BINDING_BART_TOKENIZER_ENCODER_STYLE,\n",
            "    tokenizer_bart_from_pretrained_path=MBART_TOKENIZER_BPE_BINDING_BART_TOKENIZER_ENCODER_FROM_PRETRAINED\n",
            ")\n",
            "en_bart_tokenizer = CustomBartTokenizer().build(\n",
            "    tokenizer_style=MBART_TOKENIZER_BPE_BINDING_BART_TOKENIZER_DECODER_STYLE,\n",
            "    tokenizer_bart_from_pretrained_path=MBART_TOKENIZER_BPE_BINDING_BART_TOKENIZER_DECODER_FROM_PRETRAINED\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e43157d2-c90a-4a82-a8fd-f4c9256ca9fe",
         "metadata": {},
         "outputs": [],
         "source": [
            "print(f\"Hinglish tokenizer vocab size: {hi_en_bart_tokenizer.vocab_size}\")\n",
            "print(f\"English tokenizer vocab size: {en_bart_tokenizer.vocab_size}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ea448cc8-4d09-40e8-a793-db3b7c6d59d7",
         "metadata": {},
         "source": [
            "## Dataset and Data Loaders"
         ]
      },
      {
         "cell_type": "raw",
         "id": "06950526-374c-4bb2-834d-db2e62f06e56",
         "metadata": {},
         "source": [
            "# Get the data loaders for denoising\n",
            "__data_loaders__ = get_data_loader_models(\n",
            "    train_df=train_df,\n",
            "    validation_df=validation_df,\n",
            "    test_df=test_df,\n",
            "    train_batch_size=4, \n",
            "    validation_batch_size=4, \n",
            "    test_batch_size=4,\n",
            "    encoder_tokenizer=hi_en_bart_tokenizer,\n",
            "    decoder_tokenizer=hi_en_bart_tokenizer,\n",
            "    denoising_stage=True\n",
            ")"
         ]
      },
      {
         "cell_type": "raw",
         "id": "a5d60d63-290c-4fae-8662-4249e472539d",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(dict,\n",
                     "            {'hi_en__en': {'object': <src.machine_translation.data.CodeMixedDataLoader at 0x1d7d553f488>,\n",
                     "              'train': <torch.utils.data.dataloader.DataLoader at 0x1d7d553f608>,\n",
                     "              'validation': <torch.utils.data.dataloader.DataLoader at 0x1d7c6e64448>,\n",
                     "              'test': <torch.utils.data.dataloader.DataLoader at 0x1d7d553f548>}})"
                  ]
               },
               "execution_count": 19,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "__data_loaders__"
         ]
      },
      {
         "cell_type": "raw",
         "id": "7551a750-02fd-4bb8-9bc9-a174a17c1f19",
         "metadata": {},
         "source": [
            "__data_loader__, __train_data_loader__, __validation_data_loader__, __test_data_loader__ = __data_loaders__[\"hi_en__en\"].values()"
         ]
      },
      {
         "cell_type": "raw",
         "id": "22fa8491-7d4e-4f9b-ac91-27b4528b9ccc",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "####################################################################################################\n",
                  "Train Dataloader\n",
                  "Batch Size:  4\n",
                  "Number of batches:  43611\n",
                  "Batch source language shape:  torch.Size([4, 12])\n",
                  "Batch source language:  ['craig ferguson ne kaun sa bhaag diya?', 'is week me kitne dino ko baarish hone wali hai', 'phil collins san francisco me agla time par kab hoga', 'sarey weekly reminders remove kardo']\n",
                  "Batch source tokens:  tensor([[    0, 14928, 10826,   455,  1137,   541,  4929,  1119,    34,     2,\n",
                  "             1,     1],\n",
                  "        [    0,   299,   422,   284,   551,  2072,   280,   707,   460,   819,\n",
                  "           278,     2],\n",
                  "        [    0,  2742,  6787,   877,  1440,   284,  1477,   458,   337,   511,\n",
                  "           598,     2],\n",
                  "        [    0,  3216,  1216,   493,  1784,   672,     2,     1,     1,     1,\n",
                  "             1,     1]])\n",
                  "Batch source attention mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
                  "Batch target language shape:  torch.Size([4, 12])\n",
                  "Batch target language:  ['craig ferguson ne kaun sa bhaag diya?', 'is week me kitne dino ko baarish hone wali hai', 'phil collins san francisco me agla time par kab hoga', 'sarey weekly reminders remove kardo']\n",
                  "Batch target tokens:  tensor([[    0, 14928, 10826,   455,  1137,   541,  4929,  1119,    34,     2,\n",
                  "             1,     1],\n",
                  "        [    0,   299,   422,   284,   551,  2072,   280,   707,   460,   819,\n",
                  "           278,     2],\n",
                  "        [    0,  2742,  6787,   877,  1440,   284,  1477,   458,   337,   511,\n",
                  "           598,     2],\n",
                  "        [    0,  3216,  1216,   493,  1784,   672,     2,     1,     1,     1,\n",
                  "             1,     1]])\n",
                  "Batch target attention mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
                  "Validating train laoder...\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|██████████| 43611/43611 [00:26<00:00, 1646.30it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Validation of train loader successful.\n",
                  "####################################################################################################\n",
                  "Val Dataloader\n",
                  "Batch Size:  4\n",
                  "Number of batches:  811\n",
                  "Validating validation laoder...\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|██████████| 811/811 [00:13<00:00, 60.29it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Validation of validation loader successful.\n",
                  "####################################################################################################\n",
                  "Test Dataloader\n",
                  "Batch Size:  4\n",
                  "Number of batches:  1768\n",
                  "Validating test laoder...\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|██████████| 1768/1768 [00:12<00:00, 139.69it/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Validation of test loader successful.\n",
                  "####################################################################################################\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            }
         ],
         "source": [
            "__data_loader__.visualize()"
         ]
      },
      {
         "cell_type": "raw",
         "id": "c4b0e9c2-655f-4a08-909e-59a8fea598e0",
         "metadata": {},
         "source": [
            "# Get the data loaders for translation\n",
            "__data_loaders__ = get_data_loader_models(\n",
            "    train_df=train_df,\n",
            "    validation_df=validation_df,\n",
            "    test_df=test_df,\n",
            "    train_batch_size=4, \n",
            "    validation_batch_size=4, \n",
            "    test_batch_size=4,\n",
            "    encoder_tokenizer=hi_en_bart_tokenizer,\n",
            "    decoder_tokenizer=en_bart_tokenizer\n",
            ")"
         ]
      },
      {
         "cell_type": "raw",
         "id": "96a12e0b-62e6-44c1-9219-229aa51bc57d",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(dict,\n",
                     "            {'hi_en__en': {'object': <src.machine_translation.data.CodeMixedDataLoader at 0x1d7d2be8c48>,\n",
                     "              'train': <torch.utils.data.dataloader.DataLoader at 0x1d7c6f7c788>,\n",
                     "              'validation': <torch.utils.data.dataloader.DataLoader at 0x1d7d2f2f488>,\n",
                     "              'test': <torch.utils.data.dataloader.DataLoader at 0x1d7c9ef1c88>}})"
                  ]
               },
               "execution_count": 23,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "__data_loaders__"
         ]
      },
      {
         "cell_type": "raw",
         "id": "e22633de-1352-482e-b8eb-8d450200b318",
         "metadata": {},
         "source": [
            "__data_loader__, __train_data_loader__, __validation_data_loader__, __test_data_loader__ = __data_loaders__[\"hi_en__en\"].values()"
         ]
      },
      {
         "cell_type": "raw",
         "id": "8268f86f-3b58-412c-bb5d-8d540c7b6365",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "####################################################################################################\n",
                  "Train Dataloader\n",
                  "Batch Size:  4\n",
                  "Number of batches:  43611\n",
                  "Batch source language shape:  torch.Size([4, 10])\n",
                  "Batch source language:  ['doctor ke liye reminder cancel kare', 'poland mein kaisa dikhraha hai ?', '70 ke rock ko bajao', 'theek hai, aapako ise svayan dekhana hoga']\n",
                  "Batch source tokens:  tensor([[    0,  3998,   271,   283,   347,   605,   315,     2,     1,     1],\n",
                  "        [    0, 15497,   509,   428,  4118,   278,   318,     2,     1,     1],\n",
                  "        [    0,  5287,   271,  1324,   280,   549,     2,     1,     1,     1],\n",
                  "        [    0,  7018,   278,    15,  3518,  1183, 33492,  4156,   598,     2]])\n",
                  "Batch source attention mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
                  "Batch target language shape:  torch.Size([4, 12])\n",
                  "Batch target language:  ['cancel reminder about doctor', \"what ' s it looking like in poland ?\", \"play 70 ' s rock\", 'well, you have to see it yourself']\n",
                  "Batch target tokens:  tensor([[    0,   438, 43970,  8306,    59,  3299,     2,     1,     1,     1,\n",
                  "             1,     1],\n",
                  "        [    0, 12196,   128,   579,    24,   546,   101,    11,  8385,   463,\n",
                  "         17487,     2],\n",
                  "        [    0,  5785,  1510,   128,   579,  3152,     2,     1,     1,     1,\n",
                  "             1,     1],\n",
                  "        [    0,  3056,     6,    47,    33,     7,   192,    24,  2512,     2,\n",
                  "             1,     1]])\n",
                  "Batch target attention mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
                  "Validating train laoder...\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|██████████| 43611/43611 [00:29<00:00, 1463.25it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Validation of train loader successful.\n",
                  "####################################################################################################\n",
                  "Val Dataloader\n",
                  "Batch Size:  4\n",
                  "Number of batches:  811\n",
                  "Validating validation laoder...\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|██████████| 811/811 [00:14<00:00, 57.42it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Validation of validation loader successful.\n",
                  "####################################################################################################\n",
                  "Test Dataloader\n",
                  "Batch Size:  4\n",
                  "Number of batches:  1768\n",
                  "Validating test laoder...\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|██████████| 1768/1768 [00:14<00:00, 124.94it/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Validation of test loader successful.\n",
                  "####################################################################################################\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            }
         ],
         "source": [
            "__data_loader__.visualize()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f96d610c-39a2-4c44-8356-3cae031dc975",
         "metadata": {},
         "source": [
            "## Model definition"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e8fa45c5-e36f-40ac-8739-d05f225a40a0",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "BartForConditionalGeneration(\n",
                     "  (model): BartModel(\n",
                     "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
                     "    (encoder): BartEncoder(\n",
                     "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
                     "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
                     "      (layers): ModuleList(\n",
                     "        (0): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (1): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (2): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (3): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (4): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (5): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (6): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (7): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (8): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (9): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (10): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (11): BartEncoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "    )\n",
                     "    (decoder): BartDecoder(\n",
                     "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
                     "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
                     "      (layers): ModuleList(\n",
                     "        (0): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (1): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (2): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (3): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (4): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (5): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (6): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (7): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (8): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (9): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (10): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "        (11): BartDecoderLayer(\n",
                     "          (self_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (activation_fn): GELUActivation()\n",
                     "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (encoder_attn): BartAttention(\n",
                     "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                     "          )\n",
                     "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                     "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                     "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "        )\n",
                     "      )\n",
                     "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                     "    )\n",
                     "  )\n",
                     "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
                     ")"
                  ]
               },
               "execution_count": 26,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "__model__ = BartForConditionalGeneration()\n",
            "__model__.model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "id": "39520ff4-165a-42e8-af2d-cc0b7d958c51",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Model has a total of 406291456 number of parameters\n",
                  "Model encoder input size:  torch.Size([8, 17])\n",
                  "Model decoder input size:  torch.Size([8, 21])\n",
                  "Model output type:  <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
                  "Model output size:  torch.Size([8, 21, 50265])\n",
                  "Model output:\n",
                  " tensor([[[ 13.8037,  -1.5430,   5.5749,  ...,  -2.0705,  -2.5284,   3.1019],\n",
                  "         [-30.4389,  -2.7391,   5.1244,  ...,  -1.2274,  -0.7405,   2.6097],\n",
                  "         [ -6.6546,  -2.7839,   7.6908,  ...,  -1.7490,  -2.9420,   2.4142],\n",
                  "         ...,\n",
                  "         [ -5.3017,  -3.2074,   2.9960,  ...,  -2.9747,  -3.6109,  -2.5332],\n",
                  "         [ -8.5728,  -2.7164,   4.3671,  ...,  -1.2633,  -2.4995,  -0.4308],\n",
                  "         [ -9.6214,  -1.7396,  11.3250,  ...,   2.2406,  -1.3513,   6.8301]],\n",
                  "\n",
                  "        [[ 17.2651,  -1.1422,   7.5051,  ...,  -0.1251,  -1.6244,   3.7492],\n",
                  "         [ -9.4207,  -2.9711,   4.2016,  ...,  -1.6579,  -2.4945,  -1.5065],\n",
                  "         [ -0.0342,  -1.3373,   5.1807,  ...,   0.2894,  -0.2027,   1.0067],\n",
                  "         ...,\n",
                  "         [ -2.5533,  -2.0078,   8.8614,  ...,  -1.8765,  -1.4092,   0.8992],\n",
                  "         [ -1.4687,  -2.0425,   9.1770,  ...,  -2.1458,  -1.7075,   1.0112],\n",
                  "         [ -1.7281,  -2.0395,   9.0525,  ...,  -2.1085,  -1.6606,   1.0124]],\n",
                  "\n",
                  "        [[ 17.5030,  -1.2601,   8.8174,  ...,   0.3658,  -0.3776,   3.7878],\n",
                  "         [-26.8390,  -2.8354,   2.2624,  ...,  -0.5184,  -0.8503,  -0.6993],\n",
                  "         [-10.0074,  -3.0970,   3.8391,  ...,  -1.7269,  -2.3693,  -1.1369],\n",
                  "         ...,\n",
                  "         [ -6.6480,  -2.9170,   3.9230,  ...,  -1.7310,  -2.5104,  -3.2087],\n",
                  "         [-15.2985,  -2.9173,   7.3028,  ...,  -1.3823,  -2.9498,   1.9480],\n",
                  "         [ -6.3058,  -2.9515,   4.6942,  ...,  -1.5356,  -2.9058,  -2.2722]],\n",
                  "\n",
                  "        ...,\n",
                  "\n",
                  "        [[ 13.7436,  -1.1622,   6.1653,  ...,   0.0412,  -0.5521,   4.0212],\n",
                  "         [-14.5721,  -2.1955,   3.1337,  ...,  -0.4412,  -1.0550,   1.9857],\n",
                  "         [ -9.1553,  -2.9361,   2.6973,  ...,  -1.7641,  -2.5806,   0.1015],\n",
                  "         ...,\n",
                  "         [ -0.9646,  -2.6025,   5.4699,  ...,  -0.9795,  -1.6397,  -1.8422],\n",
                  "         [ -0.9254,  -2.5897,   5.5053,  ...,  -0.9481,  -1.5954,  -1.8414],\n",
                  "         [  2.1065,  -1.9921,   5.9689,  ...,  -0.3058,  -0.8750,   0.3318]],\n",
                  "\n",
                  "        [[ 15.6766,  -1.4406,   8.4340,  ...,   0.7791,   0.5909,   3.9545],\n",
                  "         [-25.8194,  -2.4646,  12.7710,  ...,  -2.3494,  -1.2381,   3.3494],\n",
                  "         [ -4.3635,  -3.3483,  10.3594,  ...,  -2.3774,  -3.1406,   0.5130],\n",
                  "         ...,\n",
                  "         [ -4.0970,  -3.1795,   4.5883,  ...,  -1.7044,  -2.7073,  -3.5536],\n",
                  "         [ -8.3190,  -3.3910,   5.2628,  ...,  -2.2996,  -3.0002,  -4.7152],\n",
                  "         [ -5.0209,  -3.3251,   5.6923,  ...,  -1.5084,  -3.0149,  -3.4179]],\n",
                  "\n",
                  "        [[ 18.7163,  -1.7853,   8.8104,  ...,  -2.5156,  -2.4498,   1.9230],\n",
                  "         [-17.6992,  -2.5373,   3.7591,  ...,  -1.4939,  -1.4483,  -1.8701],\n",
                  "         [-11.5268,  -3.1152,   4.8946,  ...,  -2.9488,  -3.0876,  -1.7761],\n",
                  "         ...,\n",
                  "         [ -7.8463,  -2.9319,   4.0658,  ...,  -1.0043,  -0.7013,  -2.1373],\n",
                  "         [  0.7562,  -1.9806,   7.2956,  ...,  -0.6240,  -2.2554,   1.9590],\n",
                  "         [ -0.3787,  -2.2003,   5.7702,  ...,  -1.2988,  -3.0600,   1.3812]]],\n",
                  "       device='cuda:0', grad_fn=<AddBackward0>)\n"
               ]
            }
         ],
         "source": [
            "__batch_size__ = 8\n",
            "__en_seq_length__ = random.randint(13, 23)\n",
            "__de_seq_length__ = random.randint(13, 23)\n",
            "__encoder_input__ = torch.randint(low=0, high=hi_en_bart_tokenizer.vocab_size, size=(__batch_size__, __en_seq_length__))\n",
            "__decoder_input__ = torch.randint(low=0, high=en_bart_tokenizer.vocab_size, size=(__batch_size__, __de_seq_length__))\n",
            "__encoder_input__ = __encoder_input__.to(\"cuda\")\n",
            "__decoder_input__ = __decoder_input__.to(\"cuda\")\n",
            "__out__ = __model__.model(input_ids=__encoder_input__, decoder_input_ids=__decoder_input__, return_dict=True)\n",
            "print(f\"Model has a total of {__model__.model.num_parameters()} number of parameters\")\n",
            "print(\"Model encoder input size: \", __encoder_input__.size())\n",
            "print(\"Model decoder input size: \", __decoder_input__.size())\n",
            "print(\"Model output type: \", type(__out__))\n",
            "print(\"Model output size: \", __out__.logits.size())\n",
            "print(\"Model output:\\n\", __out__.logits)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 64,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_set = get_tokenized_dataset(\n",
            "    train_df=train_df,\n",
            "    validation_df=validation_df,\n",
            "    test_df=test_df,\n",
            "    encoder_tokenizer=hi_en_bart_tokenizer,\n",
            "    decoder_tokenizer=en_bart_tokenizer\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 65,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(dict,\n",
                     "            {'hi_en__en': {'train': <src.machine_translation.data.CodeMixedTokenizedDataset at 0x1d7d8657308>,\n",
                     "              'validation': <src.machine_translation.data.CodeMixedTokenizedDataset at 0x1d7d8657d88>,\n",
                     "              'test': <src.machine_translation.data.CodeMixedTokenizedDataset at 0x1d7d8657208>}})"
                  ]
               },
               "execution_count": 65,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "data_set"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 66,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_dataset, validation_dataset, test_dataset = data_set[\"hi_en__en\"].values()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 67,
         "metadata": {},
         "outputs": [],
         "source": [
            "mt_model_HG_Trainer = CodeMixedModelHGTrainer(\n",
            "    train_dataset = train_dataset,\n",
            "    validation_dataset = validation_dataset,\n",
            "    test_dataset = test_dataset\n",
            "    trainable_layers=[\n",
            "        \"model.shared.weight\",\n",
            "        # \"model.encoder.embed_positions.weight\",\n",
            "        # \"model.decoder.embed_positions.weight\"\n",
            "    ]\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 69,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Loading model...\n"
               ]
            },
            {
               "ename": "RuntimeError",
               "evalue": "CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 8.00 GiB total capacity; 7.26 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch)",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                  "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29720\\235430907.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrained_mt_model_HG_Trainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_trained_mt_model_HG_Trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmt_model_HG_Trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
                  "\u001b[1;32mc:\\Users\\tejam\\OneDrive\\Documents\\Code-Mixed-Machine-Translation\\src\\machine_translation\\net.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbest\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         '''\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_configure_optimizers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                  "\u001b[1;32mc:\\Users\\tejam\\OneDrive\\Documents\\Code-Mixed-Machine-Translation\\src\\machine_translation\\net.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                  "\u001b[1;32mc:\\Users\\tejam\\OneDrive\\Documents\\Code-Mixed-Machine-Translation\\src\\machine_translation\\net.py\u001b[0m in \u001b[0;36m_get_model\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m     76\u001b[0m             return BartForConditionalGeneration(\n\u001b[0;32m     77\u001b[0m                 \u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_pretrained\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mpretrained_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             ).model\n\u001b[0;32m     80\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                  "\u001b[1;32mc:\\Users\\tejam\\OneDrive\\Documents\\Code-Mixed-Machine-Translation\\src\\machine_translation\\models\\bart_conditional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, pretrained, pretrained_path, device)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                  "\u001b[1;32mc:\\Users\\tejam\\OneDrive\\Documents\\Code-Mixed-Machine-Translation\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1900\u001b[0m             )\n\u001b[0;32m   1901\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                  "\u001b[1;32mc:\\Users\\tejam\\OneDrive\\Documents\\Code-Mixed-Machine-Translation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     def register_backward_hook(\n",
                  "\u001b[1;32mc:\\Users\\tejam\\OneDrive\\Documents\\Code-Mixed-Machine-Translation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                  "\u001b[1;32mc:\\Users\\tejam\\OneDrive\\Documents\\Code-Mixed-Machine-Translation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                  "\u001b[1;32mc:\\Users\\tejam\\OneDrive\\Documents\\Code-Mixed-Machine-Translation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                  "\u001b[1;32mc:\\Users\\tejam\\OneDrive\\Documents\\Code-Mixed-Machine-Translation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                  "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 8.00 GiB total capacity; 7.26 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch)"
               ]
            }
         ],
         "source": [
            "trained_mt_model_HG_Trainer, best_trained_mt_model_HG_Trainer = mt_model_HG_Trainer.fit()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2680e45e-feb9-470d-ab00-5c2fd79f12a5",
         "metadata": {},
         "source": [
            "## Model Training"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 34,
         "id": "7f53f7d8-b0c1-4708-86b6-392c96199678",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(dict,\n",
                     "            {'hi_en__en': {'object': <src.machine_translation.data.CodeMixedDataLoader at 0x1d7da30f5c8>,\n",
                     "              'train': <torch.utils.data.dataloader.DataLoader at 0x1d7da30f0c8>,\n",
                     "              'validation': <torch.utils.data.dataloader.DataLoader at 0x1d7da30f308>,\n",
                     "              'test': <torch.utils.data.dataloader.DataLoader at 0x1d7da30fbc8>}})"
                  ]
               },
               "execution_count": 34,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "data_loaders = get_data_loader_models(\n",
            "    train_df=train_df,\n",
            "    validation_df=validation_df,\n",
            "    test_df=test_df,\n",
            "    encoder_tokenizer=hi_en_bart_tokenizer,\n",
            "    decoder_tokenizer=en_bart_tokenizer\n",
            ")\n",
            "data_loaders"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "aa40f729-86df-420c-8ca4-0db91a27efb7",
         "metadata": {},
         "outputs": [],
         "source": [
            "data_loader, train_data_laoder, validation_data_loader, test_data_loader = data_loaders[\"hi_en__en\"].values()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 30,
         "id": "e78bebb2-7b98-488a-9652-00312f4ae784",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Loading model...\n",
                  "No saved model to load as `saved_model_path` was not provided in the `__init__()`...\n",
                  "Freezing the model...\n",
                  "BartForConditionalGeneration(\n",
                  "  (model): BartModel(\n",
                  "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
                  "    (encoder): BartEncoder(\n",
                  "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
                  "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
                  "      (layers): ModuleList(\n",
                  "        (0): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (1): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (2): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (3): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (4): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (5): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (6): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (7): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (8): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (9): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (10): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (11): BartEncoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "      )\n",
                  "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "    )\n",
                  "    (decoder): BartDecoder(\n",
                  "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
                  "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
                  "      (layers): ModuleList(\n",
                  "        (0): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (1): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (2): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (3): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (4): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (5): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (6): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (7): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (8): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (9): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (10): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "        (11): BartDecoderLayer(\n",
                  "          (self_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (activation_fn): GELUActivation()\n",
                  "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (encoder_attn): BartAttention(\n",
                  "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
                  "          )\n",
                  "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
                  "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
                  "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "        )\n",
                  "      )\n",
                  "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
                  "    )\n",
                  "  )\n",
                  "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
                  ")\n",
                  "\n",
                  "Configuring optimizer and scheduler...\n",
                  "\n",
                  "Configuring criterion...\n",
                  "\n",
                  "Training the model...\n",
                  "\n",
                  "DEVICE - cuda || EPOCHS - 50 || LEARNING RATE - 0.0.\n",
                  "\n",
                  "EPOCH - 1/50 || START AT - 22:22:41 24|11|2023 || LEARNING RATE - 0.0\n",
                  "\n",
                  "\tTrain Step - 100/174443 | Train Step Loss: 9.41244 | Time: 20.75s.\n",
                  "\n",
                  "\tTrain Step - 200/174443 | Train Step Loss: 9.71173 | Time: 6.92s.\n",
                  "\n",
                  "\tTrain Step - 300/174443 | Train Step Loss: 9.54086 | Time: 7.10s.\n",
                  "\n",
                  "\tTrain Step - 400/174443 | Train Step Loss: 9.47746 | Time: 7.39s.\n",
                  "\n",
                  "\tTrain Step - 500/174443 | Train Step Loss: 9.55957 | Time: 7.19s.\n",
                  "\n",
                  "\tTrain Step - 600/174443 | Train Step Loss: 9.40508 | Time: 6.84s.\n",
                  "\n",
                  "\tTrain Step - 700/174443 | Train Step Loss: 9.63518 | Time: 6.82s.\n",
                  "\n",
                  "\tTrain Step - 800/174443 | Train Step Loss: 9.31375 | Time: 6.75s.\n",
                  "\n",
                  "\tTrain Step - 900/174443 | Train Step Loss: 9.65261 | Time: 6.84s.\n",
                  "\n",
                  "\tTrain Step - 1000/174443 | Train Step Loss: 9.78513 | Time: 6.80s.\n",
                  "\n",
                  "\tTrain Step - 1100/174443 | Train Step Loss: 9.56279 | Time: 6.90s.\n",
                  "\n",
                  "\tTrain Step - 1200/174443 | Train Step Loss: 9.27500 | Time: 7.18s.\n",
                  "\n",
                  "\tTrain Step - 1300/174443 | Train Step Loss: 9.68104 | Time: 7.01s.\n",
                  "\n",
                  "\tTrain Step - 1400/174443 | Train Step Loss: 9.88009 | Time: 7.13s.\n",
                  "\n",
                  "\tTrain Step - 1500/174443 | Train Step Loss: 9.50213 | Time: 7.15s.\n",
                  "\n",
                  "\tTrain Step - 1600/174443 | Train Step Loss: 9.73859 | Time: 6.95s.\n",
                  "\n",
                  "\tTrain Step - 1700/174443 | Train Step Loss: 9.73458 | Time: 6.86s.\n",
                  "\n",
                  "\tTrain Step - 1800/174443 | Train Step Loss: 9.16421 | Time: 6.98s.\n",
                  "\n",
                  "\tTrain Step - 1900/174443 | Train Step Loss: 9.56582 | Time: 6.99s.\n",
                  "\n",
                  "\tTrain Step - 2000/174443 | Train Step Loss: 9.52915 | Time: 7.15s.\n",
                  "\n",
                  "\tTrain Step - 2100/174443 | Train Step Loss: 9.45163 | Time: 7.17s.\n",
                  "\n",
                  "\tTrain Step - 2200/174443 | Train Step Loss: 9.41697 | Time: 7.26s.\n",
                  "\n",
                  "\tTrain Step - 2300/174443 | Train Step Loss: 9.65010 | Time: 7.34s.\n",
                  "\n",
                  "\tTrain Step - 2400/174443 | Train Step Loss: 9.43881 | Time: 7.46s.\n",
                  "\n",
                  "\tTrain Step - 2500/174443 | Train Step Loss: 9.28909 | Time: 7.37s.\n",
                  "\n",
                  "\tTrain Step - 2600/174443 | Train Step Loss: 9.83027 | Time: 7.58s.\n",
                  "\n",
                  "\tTrain Step - 2700/174443 | Train Step Loss: 9.40395 | Time: 7.62s.\n",
                  "\n",
                  "\tTrain Step - 2800/174443 | Train Step Loss: 9.32339 | Time: 11.41s.\n",
                  "\n",
                  "\tTrain Step - 2900/174443 | Train Step Loss: 9.43646 | Time: 15.77s.\n",
                  "\n",
                  "\tTrain Step - 3000/174443 | Train Step Loss: 9.72062 | Time: 14.73s.\n",
                  "\n",
                  "\tTrain Step - 3100/174443 | Train Step Loss: 9.51878 | Time: 12.90s.\n",
                  "\n",
                  "\tTrain Step - 3200/174443 | Train Step Loss: 9.47551 | Time: 14.88s.\n",
                  "\n",
                  "\tTrain Step - 3300/174443 | Train Step Loss: 9.65120 | Time: 14.08s.\n",
                  "\n",
                  "\tTrain Step - 3400/174443 | Train Step Loss: 9.38361 | Time: 14.27s.\n",
                  "\n",
                  "\tTrain Step - 3500/174443 | Train Step Loss: 9.42329 | Time: 13.65s.\n",
                  "\n",
                  "\tTrain Step - 3600/174443 | Train Step Loss: 9.83736 | Time: 13.38s.\n",
                  "\n",
                  "\tTrain Step - 3700/174443 | Train Step Loss: 9.64917 | Time: 14.50s.\n",
                  "\n",
                  "\tTrain Step - 3800/174443 | Train Step Loss: 9.41333 | Time: 11.77s.\n",
                  "\n",
                  "\tTrain Step - 3900/174443 | Train Step Loss: 9.50755 | Time: 12.54s.\n",
                  "\n",
                  "\tTrain Step - 4000/174443 | Train Step Loss: 9.38311 | Time: 12.46s.\n",
                  "\n",
                  "\tTrain Step - 4100/174443 | Train Step Loss: 9.37791 | Time: 12.33s.\n",
                  "\n",
                  "\tTrain Step - 4200/174443 | Train Step Loss: 9.69934 | Time: 12.07s.\n",
                  "\n",
                  "\tTrain Step - 4300/174443 | Train Step Loss: 9.40349 | Time: 12.02s.\n",
                  "\n",
                  "\tTrain Step - 4400/174443 | Train Step Loss: 9.41872 | Time: 11.92s.\n",
                  "\n",
                  "\tTrain Step - 4500/174443 | Train Step Loss: 9.43612 | Time: 12.28s.\n",
                  "\n",
                  "\tTrain Step - 4600/174443 | Train Step Loss: 9.50006 | Time: 12.70s.\n",
                  "\n",
                  "\tTrain Step - 4700/174443 | Train Step Loss: 9.75746 | Time: 12.42s.\n",
                  "\n",
                  "\tTrain Step - 4800/174443 | Train Step Loss: 9.63050 | Time: 12.35s.\n",
                  "\n",
                  "\tTrain Step - 4900/174443 | Train Step Loss: 9.50375 | Time: 12.39s.\n",
                  "\n",
                  "\tTrain Step - 5000/174443 | Train Step Loss: 9.41272 | Time: 14.78s.\n",
                  "\n",
                  "\tTrain Step - 5100/174443 | Train Step Loss: 9.55126 | Time: 15.24s.\n",
                  "\n",
                  "\tTrain Step - 5200/174443 | Train Step Loss: 9.62946 | Time: 13.25s.\n",
                  "\n",
                  "\tTrain Step - 5300/174443 | Train Step Loss: 9.80079 | Time: 15.14s.\n",
                  "\n",
                  "\tTrain Step - 5400/174443 | Train Step Loss: 9.53567 | Time: 13.77s.\n",
                  "\n",
                  "\tTrain Step - 5500/174443 | Train Step Loss: 9.73552 | Time: 14.73s.\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "mt_model = CodeMixedModel(\n",
            "    train_data_loader=train_data_laoder,\n",
            "    validation_data_loader=validation_data_loader,\n",
            "    test_data_loader=test_data_loader,\n",
            "    encoder_tokenizer=hi_en_bart_tokenizer,\n",
            "    decoder_tokenizer=en_bart_tokenizer,\n",
            "    trainable_layers=[\n",
            "        \"model.shared.weight\",\n",
            "        # \"model.encoder.embed_positions.weight\",\n",
            "        # \"model.decoder.embed_positions.weight\"\n",
            "    ]\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 60,
         "id": "552df5e7-e547-4ce2-b432-587345d072fd",
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "torch.cuda.empty_cache()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 61,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "|===========================================================================|\n",
                  "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
                  "|---------------------------------------------------------------------------|\n",
                  "|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\n",
                  "|===========================================================================|\n",
                  "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
                  "|---------------------------------------------------------------------------|\n",
                  "| Allocated memory      |    7432 MB |    7432 MB |    7614 MB |  186779 KB |\n",
                  "|       from large pool |    7251 MB |    7251 MB |    7283 MB |   32986 KB |\n",
                  "|       from small pool |     181 MB |     181 MB |     331 MB |  153792 KB |\n",
                  "|---------------------------------------------------------------------------|\n",
                  "| Active memory         |    7432 MB |    7432 MB |    7614 MB |  186779 KB |\n",
                  "|       from large pool |    7251 MB |    7251 MB |    7283 MB |   32986 KB |\n",
                  "|       from small pool |     181 MB |     181 MB |     331 MB |  153792 KB |\n",
                  "|---------------------------------------------------------------------------|\n",
                  "| GPU reserved memory   |    7482 MB |    7482 MB |    7482 MB |       0 B  |\n",
                  "|       from large pool |    7298 MB |    7298 MB |    7298 MB |       0 B  |\n",
                  "|       from small pool |     184 MB |     184 MB |     184 MB |       0 B  |\n",
                  "|---------------------------------------------------------------------------|\n",
                  "| Non-releasable memory |   51089 KB |   67621 KB |    2955 MB |    2905 MB |\n",
                  "|       from large pool |   48105 KB |   64489 KB |    2675 MB |    2628 MB |\n",
                  "|       from small pool |    2984 KB |   13155 KB |     279 MB |     276 MB |\n",
                  "|---------------------------------------------------------------------------|\n",
                  "| Allocations           |    2814    |    2814    |    3144    |     330    |\n",
                  "|       from large pool |     924    |     924    |     925    |       1    |\n",
                  "|       from small pool |    1890    |    1890    |    2219    |     329    |\n",
                  "|---------------------------------------------------------------------------|\n",
                  "| Active allocs         |    2814    |    2814    |    3144    |     330    |\n",
                  "|       from large pool |     924    |     924    |     925    |       1    |\n",
                  "|       from small pool |    1890    |    1890    |    2219    |     329    |\n",
                  "|---------------------------------------------------------------------------|\n",
                  "| GPU reserved segments |     448    |     448    |     448    |       0    |\n",
                  "|       from large pool |     356    |     356    |     356    |       0    |\n",
                  "|       from small pool |      92    |      92    |      92    |       0    |\n",
                  "|---------------------------------------------------------------------------|\n",
                  "| Non-releasable allocs |      62    |     100    |     447    |     385    |\n",
                  "|       from large pool |      18    |      19    |     172    |     154    |\n",
                  "|       from small pool |      44    |      92    |     275    |     231    |\n",
                  "|===========================================================================|\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.7.6"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}